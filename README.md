

# EXP 5: COMPARATIVE ANALYSIS OF DIFFERENT TYPES OF PROMPTING PATTERNS AND EXPLAIN WITH VARIOUS TEST SCENARIOS

# Aim: To test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios.  Analyze the quality, accuracy, and depth of the generated responses 

### AI Tools Required: 
 ChatGPT (GPT-5)

# Explanation: 
Define the Two Prompt Types:

i)Naïve Prompt (Unstructured):

Broad, unclear, or open-ended prompt.

Provides little to no context or instructions.

Example: “Tell me about AI.”

ii)Basic Prompt (Structured & Refined):

Clear, detailed, and specific instructions.

Gives context, constraints, and expected format.

Example: “Explain Artificial Intelligence in 150 words, covering definition, applications, and one limitation, in simple terms for a beginner.”

<img width="1509" height="735" alt="image" src="https://github.com/user-attachments/assets/b17ff8be-b2f6-42c5-a0b4-ad1b14f2c48e" />

Evaluation of Responses

Quality: Structured prompts produced clear, complete, and well-organized outputs compared to naïve prompts, which were often vague or incomplete.

Accuracy: Basic prompts reduced the chance of irrelevant or partially correct answers by guiding the model on scope.

Depth: Basic prompts encouraged detailed and contextual responses, while naïve prompts often remained surface-level.

Summary of Findings

Prompt clarity directly impacts output quality.

Basic prompts consistently outperformed naïve prompts in structured tasks like factual Q&A, summarization, and recommendations.

In creative/open tasks, naïve prompts sometimes worked, but structured prompts still gave more meaningful narratives.

Best practice: Always define scope, context, format, and audience in prompts for optimal results.

# OUTPUT

The experiment was carried out by providing both Naïve (unstructured) and Basic (structured) prompts to ChatGPT across multiple scenarios. The generated responses were recorded and compared.

Scenario 1: Creative Story Generation

Naïve Prompt: “Write a story.”

Response: Produced a very short and generic story with no clear characters or theme.

Basic Prompt: “Write a 200-word short story about a child who discovers a talking robot in their backyard. Focus on adventure and friendship.”

Response: Produced a detailed, well-structured story with characters, plot, adventure, and a moral on friendship.

Scenario 2: Factual Question

Naïve Prompt: “What is photosynthesis?”

Response: Returned only a simple definition of photosynthesis.

Basic Prompt: “Explain photosynthesis in 100 words, covering definition, chemical equation, and importance for plants.”

Response: Provided a complete explanation including the chemical equation (6CO₂ + 6H₂O → C₆H₁₂O₆ + 6O₂) and importance of the process for plants and life on Earth.

Scenario 3: Summarization

Naïve Prompt: “Summarize climate change.”

Response: Long answer, lacked structure, missed solutions.

Basic Prompt: “Summarize climate change in 4–5 sentences, highlighting causes, effects, and global solutions.”

Response: Clear and concise summary mentioning greenhouse gases (causes), rising sea levels & global warming (effects), and renewable energy & policies (solutions).

Scenario 4: Advice / Recommendation

Naïve Prompt: “Suggest a book.”

Response: Gave only one book recommendation without context.

Basic Prompt: “Suggest 3 beginner-friendly books on Artificial Intelligence with author names and short descriptions.”

Response: Recommended 3 AI books with author details and short explanations, making it practical and useful

<img width="1040" height="427" alt="image" src="https://github.com/user-attachments/assets/a10d9613-3a84-48b5-8aaf-76f025bd89e4" />


# RESULT: 

The experiment was successfully executed. The comparative analysis shows that basic structured prompts produce higher-quality, accurate, and deeper responses compared to naïve unstructured prompts.
